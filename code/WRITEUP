Title:  Writeup for Project 4, Fall 2015
Date:  11/29/15
Group 16:     Name                        Email
              Rochelle (Shelly) Willard   rwillard@usc.edu
              Ananth Mohan                 ananthmo@usc.edu
              Matthew Burke                matthelb@usc.edu

I. Requirements:
Part 1: Each entity of the passport office is implemented as a separate user program. The passport simulation can be run by executing the entities as separate user program and managing the shared data on one or more servers.

Part 2: Allow for multiple servers to handle client requests.  Clients are to randomly choose a server to send their request to.  Locks, Conditions, and Monitors will be stored on the server the first CREATE request was sent to.  On a client request if the entity is not found on that server, the server must send requests to each of the other servers to find the requested entity.

II. Assumptions:
Part 1: There are a maximum of 10 mailboxes on each instance of a nachos machine, so only up to 8 separate entities can be executed on a single machine (because mailbox 0 is reserved for main and mailbox 1 is reserved for the postal office thread).


Part 2: No more than 10000 of a single entity will be requested from an individual server.  At least one server needs to be running at a time.  Each server and client needs to be passed the number of servers using the “-sn” command line argument.

III. Design:

Part 1: The code from Part 3 of Project 2 will be largely reused to execute the passport simulation. Any data that is shared between threads will be converted to monitor variables stored on the one or more servers managing the simulation. Additionally, locks and conditions must be given unique names so that the server(s) can manage the references to each lock and condition.

In order to make the code compatible with Part 3 of Project 2, we can use preprocessor directives to determine whether or not the shared data will be stored and accessed on servers or if it will be stored and accessed on the local machine.


Part 2: The code from Part 3 of Project 3 will be largely reused to execute a request upon verification of the requested entity’s location. On receiving a client request, if the requested entity (lock, condition, or monitor) is not located on that server, the server will assign a request id, send request messages with all of the necessary information to each of the other servers, and add the request id and request to a pending_request map.  

Upon receiving the server request, if a server has the requested entity it sends a ‘YES’ back to the requesting server and executes the action to be done.  If the server does not have the entity, it sends a ‘NO’ back to the requesting server.  The requesting server keeps track of if a ‘YES’ response has been received and the number of ‘NO’ responses it has received.  Upon receiving a ‘YES’ server response the requesting server is done handling the request.  Upon receiving all ‘NO’ server responses the server will in the case of a CREATE request create the requested entity and in all other cases return an error message.

For the use of Condition variables, the request path is slightly different because of the possibility that the Lock used for that Condition was created by a different server than the one the Condition variable was created on. To solve this, three new types of messages were added in the case that the CV was found on a server but its lock wasn’t: WAIT_CV_LOCK, SIGNAL_CV_LOCK, and BROADCAST_CV_LOCK. When received by a server, they check and see if they own the lock in question. If they do, then they execute the action necessary for that specific request, and then return a ‘YES’ to the server who owns the CV. The server owning the CV will then, if necessary, send a response back to the client who had originally signalled or broadcasted, as well as performing the actions necessary to maintain the wait queue of the CV.

Lock, Condition, and Monitor IDs will be assigned starting at machineId * 10000. For example, Server 1 will have Locks, Conditions, and Monitors 10000 to 19999.

IV. Implementation:
+ Files Modified
network/nettest.cc
network/network_utility.h
threads/system.cc
threads/system.h
test/simulation.c
test/utilities.h

+ Files added
test/application_clerk.c
test/cashier.c
test/customer.c
test/manager.c
test/passport_clerk.c
test/picture_clerk.c
test/senator.c
test/simulation.h


+ Data Structures added, and the file they were added to.
// network/nettest.cc

// Data structure used to hold all required information to complete a client request.
struct Request {
int requestorMID;
int requestorMB;
int requestType;
      bool yesResponse;
      int noCount;
      string info;
};

+ Data Structures modified, and the file they were added to.

+ Functions added and in which file.
// test/simulation.c
// network/nettest.cc

// Helper method used to format a server response to a server request.
void sendResponse(PacketHeader outPktHdr, MailHeader outMailHdr, int requestId, 
    bool yes);

// Creates a request, assigns a request id, adds it to the pending request map, and sends the server request to each of the other servers.
void create_request_and_send_servers(PacketHeader inPktHdr, 
    MailHeader inMailHdr, PacketHeader outPktHdr, 
    MailHeader outMailHdr, map<int, Request> & pending_requests, 
    int & currentRequest, int type, string info);

// Returns the lock id if the lock is found, -1 otherwise.
int find_lock_by_name(string lock_name);
// Returns whether or not the given lock id exists on the server.
bool find_lock_by_id(int id);

// Returns the mv ID for the given lock or -1 if not on this server.
int find_mv_by_name(string mv_name);

// Returns true if the mv is on this server, false otherwise.
bool find_mv_by_id(int id);

// Returns true if the lock is currently held by the requesting client
bool lock_owned_by_requestor(ServerLock* lock, int request_pkt, int request_mailbox);

+ Functions modified and in which file.

// The run function of the server that parses and determines how to handle requests.
// Modified to handle client requests, server requests, and server responses. 
// Calls the appropriate functions to either execute the action or send requests to other servers.
void Server();

***For details on Passport Office Simulation Functions and Classes see Project 2 writeup.  All functionality defined in that writeup was maintained with the only change being shared data stored in monitor variables on the server(s) instead of as local variables on the nachos machine.***

V. Testing:  (For each test case, you must show)
To test the distributed passport office simulation, run the following commands
nachos -server -m 0 -sn 1
nachos -m 1 -sn 1 -x mini_passport_office_net_test
To test Condition Variables with multiple servers, run the following commands in multiple windows from the network directory:
nachos -sn 2 -server -m 0
nachos -sn 2 -server -m 1
nachos -sn 2 -m 2 -x ../test/conditiontest1
nachos -sn 2 -m 3 -x ../test/conditiontest2
This shows that waiting and signalling work as distributed RPCs. Setting the random seed differently will change where the Condition and Lock variables are initially created

VI. Discussion:
+ Experiment expectation:
+ The passport office simulation should complete successfully (all customers should leave)
+ The manager thread should remain running and periodically output the collected money for the office
+ Experiment result:
+ The passport office simulation completes successfully (all customers leave)
+ The manager thread remains running and periodically outputs the collected money for the office
+ Explanation:
    + There may be some issues when using certain random seeds, but we recommend running with no random seed to observe that the simulation completes successfully.

VIII. Miscellaneous:
- Whatever else you want the grader know about your development.  You can create your own subsections headings here.
